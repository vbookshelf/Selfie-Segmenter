{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Version 4**: Added web interface.\n\n<hr>"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport cv2\n\nimport shutil\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom skimage.io import imread, imshow\nfrom skimage.transform import resize\n\n# Don't Show Warning Messages\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# set up the canvas for the subplots\nplt.figure(figsize=(7,7))\nplt.tight_layout()\nplt.axis('Off')\n\n\n# Our subplot will contain 4 rows and 4 columns\n# plt.subplot(nrows, ncols, plot_number)\n\n# image\nplt.subplot(1,2,1)\npath = '../input/matting_human_half/clip_img/1803281053/clip_00000000/1803281053-00000348.jpg'\nimage = plt.imread(path)\nplt.title('RGB Image')\nplt.imshow(image)\nplt.axis('off')\n\n\n# image\nplt.subplot(1,2,2)\npath = '../input/matting_human_half/matting/1803281053/matting_00000000/1803281053-00000348.png'\nmask = plt.imread(path)\nplt.title('RGBA Image')\nplt.imshow(mask)\nplt.axis('off')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Introduction"},{"metadata":{},"cell_type":"markdown","source":"**Objective**\n\nGiven a half-length portrait image of one person, build a model that can separate the person from the background.\n\n**Approach**\n\n- Use a Keras U-Net model architecture.\n- Resize all training images to 128x128.\n- Use only 4000 images for training.\n- Use generators and chunking to reduce RAM usage and prevent the kernel from crashing.\n- Set aside 100 images to test the model.\n\n**Dataset Info**\n\n- 24,479 RGB images in jpg format - shape: (600, 800, 3)\n- 24,479 RGBA images in png format - shape: (600, 800, 4)\n- One corrupt RGB image ( ._1803241125-00000005.jpg )\n\nRGB images have 3 channels. But RGBA images have 4 channels. The 4th channel controls the pixel transparency. It's called the alpha channel. This channel has values between 0 and 255. 0 is transparent. 255 is opaque. We can add a 4th channel to any RGB image. We can then vary the transparency of any pixel on that image by changing the values on the 4th channel. \n\nWe will train the model using RGB images. The ground truth (target) is the 4th channel of the RGBA images. Therefore, we will need to extract this 4th channel.\n\n**Web Interface**\n\nI've deployed the model as a tensorflow.js web app. You can submit a photo and get a segmented output. \n\nWeb App:<br>\nhttp://selfieseg.test.woza.work/<br>\nGithub:<br>\nhttps://github.com/vbookshelf/V1-Selfie-Segmenter\n\nThe quality of the output segmentation is not spectacular. My main reason for building the app was to understand the workflow involved in building an end-to-end web based solution that ouputs an image. The same workflow can also be used for other fun machine learning applications like style transfer. Or on a more serious note, imagine having a freely available app like this that could take as input the photo of a missing child and output an aged photo. The javascript, html and css code for the app is available on github.\n\nOkay, let's start by putting all file names into dataframes and taking a look at one image."},{"metadata":{},"cell_type":"markdown","source":"<hr>"},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_TEST_IMAGES = 100\n\nIMG_HEIGHT = 128\nIMG_WIDTH = 128\nIMG_CHANNELS = 3\n\nBATCH_SIZE = 100\n\nSAMPLE_SIZE = 4000","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Get image info"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# ==========================\n# 1. Get folder and file info\n\n# image_id\n# folder_id\n# type (image or matt)\n\n\nfolder_list = os.listdir('../input/matting_human_half/clip_img')\n\nfor i, folder_id in enumerate(folder_list):\n    \n    if i == 0:\n        \n        # get a list of images in that folder\n        path = '../input/matting_human_half/clip_img/' + str(folder_id) + '/clip_00000000/'\n        image_list = os.listdir(path)\n\n        df = pd.DataFrame(image_list, columns=['image_id'])\n\n        df['folder_id'] = folder_id\n        \n        df['num_images_in_folder'] = len(image_list)\n\n        df['type'] = 'image'\n        \n        df_img = df\n    \n\n    else:\n        \n        # get a list of images in that folder\n        path = '../input/matting_human_half/clip_img/' + str(folder_id) + '/clip_00000000/'\n        image_list = os.listdir(path)\n\n        df_1 = pd.DataFrame(image_list, columns=['image_id'])\n\n        df_1['folder_id'] = folder_id\n        \n        df_1['num_images_in_folder'] = len(image_list)\n        \n\n        df_1['type'] = 'image'\n       \n        # concat the dataframes\n        df_img = pd.concat([df_img, df_1], axis=0).reset_index(drop=True)\n\n        \n# remove a corrupt file\ndf_img = df_img[df_img['image_id'] != '._1803241125-00000005.jpg']\n\n# reset the index\ndf_img = df_img.reset_index(drop=True)\n\n\n\ndf_img.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Get mask info"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\n# 1. Get folder and file info\n\n# image_id\n# folder_id\n# type (image or matt)\n\n\nfolder_list = os.listdir('../input/matting_human_half/matting/')\n\nfor j, folder_id in enumerate(folder_list):\n    \n    if j == 0:\n        \n        # get a list of images in that folder\n        path = '../input/matting_human_half/matting/' + str(folder_id) + '/matting_00000000/'\n        image_list = os.listdir(path)\n\n        df = pd.DataFrame(image_list, columns=['mask_id'])\n\n        df['folder_id'] = folder_id\n        \n        df['num_images_in_folder'] = len(image_list)\n\n        df['type'] = 'mask'\n    \n        df_msk = df\n        \n    else:\n        \n        # get a list of images in that folder\n        path = '../input/matting_human_half/matting/' + str(folder_id) + '/matting_00000000/'\n        image_list = os.listdir(path)\n\n        df_1 = pd.DataFrame(image_list, columns=['mask_id'])\n\n        df_1['folder_id'] = folder_id\n\n        df_1['type'] = 'mask'\n    \n        # concat the dataframes\n        df_msk = pd.concat([df_msk, df_1], axis=0).reset_index(drop=True)\n\n\ndf_msk.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Num Images: ', df_img.shape[0])\nprint('Num Masks: ', df_msk.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create a merged dataframe"},{"metadata":{},"cell_type":"markdown","source":"We are merging dataframes to ensure that the order of the images and masks is the same i.e. an image and its mask are on the same row. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\n# We will use this column to merge df_images and df_masks.\n\ndef get_name(x):\n    \n    name = x.split('.')[0]\n    \n    return name\n\ndf_img['merge_col'] = df_img['image_id'].apply(get_name)\n\ndf_msk['merge_col'] = df_msk['mask_id'].apply(get_name)\n\ndf_data = pd.merge(df_img, df_msk, on='merge_col')\n\n# select only specific columns\ndf_data = df_data[['image_id', 'mask_id']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_data.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Take a look at one image and it's alpha channel"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Image\n\npath = '../input/matting_human_half/clip_img/1803242036/clip_00000000/1803242036-00000058.jpg'\n\nimage = plt.imread(path)\n\nprint(image.shape)\n\nplt.imshow(image)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mask\n\npath = '../input/matting_human_half/matting/1803242036/matting_00000000/1803242036-00000058.png'\n\nmatt = plt.imread(path)\n\nprint(image.shape)\n\nplt.imshow(matt)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Extract the alpha channel\n\nThe alpha channel is the 4th channel in an RGBA image. It controls pixel transparency. 0 is fully transparent and 255 is opaque. Setting a value between 0 and 255 varies the transparency."},{"metadata":{"trusted":true},"cell_type":"code","source":"# alpha channel\n\n# If we don't use cv2.IMREAD_UNCHANGED then the image will be read with only 3 channels.\nrgba_image = cv2.imread(path, cv2.IMREAD_UNCHANGED) \n\n# select the 4th channel\nalpha = rgba_image[:,:,3]\n\nprint(rgba_image.shape)\n\nplt.imshow(alpha)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create a test set"},{"metadata":{},"cell_type":"markdown","source":"We will create a test set consisting of 100 images."},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a test set\ndf_test = df_data.sample(NUM_TEST_IMAGES, random_state=101)\n\n# Reset the index.\ndf_test = df_test.reset_index(drop=True)\n\n# create a list of test images\ntest_images_list = list(df_test['image_id'])\n\n\n# Select only rows that are not part of the test set.\n# Note the use of ~ to execute 'not in'.\ndf_data = df_data[~df_data['image_id'].isin(test_images_list)]\n\nprint(df_data.shape)\nprint(df_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_test_split\n\n# Reduce the number of rows of df_data to speed up training.\n# Choose a random sample of rows.\ndf_data = df_data.sample(SAMPLE_SIZE, random_state=101)\n\ndf_train, df_val = train_test_split(df_data, test_size=0.15, random_state=101)\n\n\n# reset the index\ndf_train = df_train.reset_index(drop=True)\ndf_val = df_val.reset_index(drop=True)\ndf_test = df_test.reset_index(drop=True)\n\nprint(df_train.shape)\nprint(df_val.shape)\nprint(df_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Save the dataframes as compressed csv files"},{"metadata":{},"cell_type":"markdown","source":"Having csv files will allow us to use Pandas chunking to feed images into the generators. Compression is very helpful when working with huge datasets because there's only 4.9GB of disk space available in the Kaggle kernel."},{"metadata":{"trusted":true},"cell_type":"code","source":"# save the dataframes as a compressed csv files\n\ndf_train.to_csv('df_train.csv.gz', compression='gzip', index=False)\ndf_val.to_csv('df_val.csv.gz', compression='gzip', index=False)\ndf_test.to_csv('df_test.csv.gz', compression='gzip', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check if the files were saved\n!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build the Data Generators"},{"metadata":{},"cell_type":"markdown","source":"The ouput from a generator does not accumulate in memory. Each output batch overwrites the last one. This means that we can feed large amounts of data into a model without running out of RAM and crashing the kernel. There's a 13GB RAM limit when using a GPU.\n\nWe will use Pandas chunking and the compressed csv files to feed data into the generators. Using chunking simplifies the code. For example, the last batch that is fed into a generator will be smaller than the others. Pandas chunking will handle this change in batch size automatically which means that we won't need to write code to handle this condition.\n\nChunking is very useful when the csv file data is too large to be loaded into memory i.e. into a single Pandas dataframe."},{"metadata":{},"cell_type":"markdown","source":"### [ 1 ] Train Generator"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_generator(batch_size=10):\n    \n    while True:\n        \n        # load the data in chunks (batches)\n        for df in pd.read_csv('df_train.csv.gz', chunksize=batch_size):\n            \n            # get the list of images\n            image_id_list = list(df['image_id'])\n            # get a list of masks\n            mask_id_list = list(df['mask_id'])\n            \n            # Create empty X matrix - 3 channels\n            # Note: We use len(df) because the last batch will be smaller than the other batches.\n            X_train = np.zeros((len(df), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n            \n            # create empty Y matrix - 1 channel\n            Y_train = np.zeros((len(df), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n\n        \n            \n            # Create X_train\n            #================\n            \n            for i, image_id in enumerate(image_id_list):\n                \n                # select the folder_id from the list\n                folder_id = image_id.split('-')[0]\n\n                # set the path to the image\n                path = '../input/matting_human_half/clip_img/' + str(folder_id) + \\\n                '/clip_00000000/' + image_id\n\n                # read the image\n                image = cv2.imread(path)\n                \n                # resize the image\n                image = resize(image, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n                \n                # insert the image into X_train\n                X_train[i] = image\n            \n            \n            # Create Y_train\n            # ===============\n                \n            for j, mask_id in enumerate(mask_id_list):\n                \n                # select the folder_id from the list\n                folder_id = mask_id.split('-')[0]\n\n                # set the path to the mask\n                path = '../input/matting_human_half/matting/' + str(folder_id) + \\\n                '/matting_00000000/' + mask_id\n\n                # read the image\n                mask = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n\n                # select the alpha channel\n                mask = mask[:, :, 3]\n                \n                # expand dims from (800,600) to (800,600,1)\n                mask = np.expand_dims(mask, axis=-1)\n                \n                # resize the mask\n                mask = resize(mask, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n                \n                # insert the image into Y_train\n                Y_train[j] = mask\n\n            yield X_train, Y_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sanity check the train generator"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test the generator\n\n# initialize\ntrain_gen = train_generator(batch_size=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# run the generator\nX_train, Y_train = next(train_gen)\n\nprint(X_train.shape)\nprint(Y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the first image in X_train\n\nimg = X_train[1,:,:,:]\nplt.imshow(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the first mask in Y_train\n\nmsk = Y_train[1,:,:,0]\nplt.imshow(msk)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display the generator output - X_train\n\n# X_train\n\nn = 10\nfig, axs = plt.subplots(nrows=n, ncols=n, sharex=True, sharey=True, figsize=(10,10))\nfor i in range(n**2):\n    ax = axs[i // n, i % n]\n    ax.imshow(X_train[i,:,:,:], cmap=plt.cm.gray)\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display the generator output - Y_train\n\n# Y_train\n\nn = 10\nfig, axs = plt.subplots(nrows=n, ncols=n, sharex=True, sharey=True, figsize=(10,10))\nfor i in range(n**2):\n    ax = axs[i // n, i % n]\n    ax.imshow(Y_train[i,:,:,0], cmap=plt.cm.gray)\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### [ 2 ] Val Generator"},{"metadata":{"trusted":true},"cell_type":"code","source":"def val_generator(batch_size=10):\n    \n    while True:\n        \n        # load the data in chunks (batches)\n        for df in pd.read_csv('df_val.csv.gz', chunksize=batch_size):\n            \n            # get the list of images\n            image_id_list = list(df['image_id'])\n            # get a list of masks\n            mask_id_list = list(df['mask_id'])\n            \n            # Create empty X matrix - 3 channels.\n            # Note: We use len(df) because the last batch will be smaller than the other batches.\n            X_val = np.zeros((len(df), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n            \n            # create empty Y matrix - 1 channel\n            Y_val = np.zeros((len(df), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n\n        \n            \n            # Create X_val\n            #================\n            \n            for i, image_id in enumerate(image_id_list):\n                \n                # select the folder_id from the list\n                folder_id = image_id.split('-')[0]\n\n                # set the path to the image\n                path = '../input/matting_human_half/clip_img/' + str(folder_id) + \\\n                '/clip_00000000/' + image_id\n\n                # read the image\n                image = cv2.imread(path)\n                \n                # resize the image\n                image = resize(image, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n                \n                # insert the image into X_val\n                X_val[i] = image\n            \n            \n            # Create Y_val\n            # ===============\n                \n            for j, mask_id in enumerate(mask_id_list):\n                \n                # select the folder_id from the list\n                folder_id = mask_id.split('-')[0]\n\n                # set the path to the mask\n                path = '../input/matting_human_half/matting/' + str(folder_id) + \\\n                '/matting_00000000/' + mask_id\n\n                # read the image\n                mask = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n\n                # select the alpha channel\n                mask = mask[:, :, 3]\n                \n                # expand dims from (800,600) to (800,600,1)\n                mask = np.expand_dims(mask, axis=-1)\n                \n                # resize the mask\n                mask = resize(mask, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n                \n                # insert the image into Y_val\n                Y_val[j] = mask\n\n            yield X_val, Y_val","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sanity check the val generator"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test the generator\n\n# initialize\nval_gen = val_generator(batch_size=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# run the generator\nX_val, Y_val = next(val_gen)\n\nprint(X_val.shape)\nprint(Y_val.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the first image in X_val\n\nimg = X_val[1,:,:,:]\nplt.imshow(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the first mask in Y_val\n\nmsk = Y_val[1,:,:,0]\nplt.imshow(msk)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### [ 3 ] Test Generator"},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_generator(batch_size=10):\n    \n    while True:\n        \n        # load the data in chunks (batches)\n        for df in pd.read_csv('df_test.csv.gz', chunksize=batch_size):\n            \n            # get the list of images\n            image_id_list = list(df['image_id'])\n            \n            # Create empty X matrix - 3 channels.\n            # Note: We use len(df) because the last batch will be smaller than the other batches.\n            X_test = np.zeros((len(df), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n            \n\n            # Create X_test\n            #================\n            \n            for i, image_id in enumerate(image_id_list):\n                \n                # select the folder_id from the list\n                folder_id = image_id.split('-')[0]\n\n                # set the path to the image\n                path = '../input/matting_human_half/clip_img/' + str(folder_id) + \\\n                '/clip_00000000/' + image_id\n\n                # read the image\n                image = cv2.imread(path)\n                \n                # resize the image\n                image = resize(image, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n                \n                # insert the image into X_test\n                X_test[i] = image\n            \n\n            yield X_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Sanity check the test generator"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test the generator\n\n# initialize\ntest_gen = test_generator(batch_size=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# run the generator\nX_test = next(test_gen)\n\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the first image in X_test\n\nimg = X_test[1,:,:,:]\nplt.imshow(img)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create X_test"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we will use the test generator with a batch size of 1 to create X_test\n\n# initialize\ntest_gen = test_generator(batch_size=len(df_test))\n# run the generator\nX_test = next(test_gen)\n\nX_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create X_test_orig - original 800x600 size"},{"metadata":{},"cell_type":"markdown","source":"Here we will create a numpy matrix containing the test images in their original 800x600 size."},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# get the list of images\ntest_id_list = list(df_test['image_id'])\n\n# Create empty X matrix - 3 channels.\n# Note: We use len(df) because the last batch will be smaller than the other batches.\nX_test_orig = np.zeros((len(df_test), 800, 600, 3), dtype=np.uint8)\n\n\n# Create X_test\n#================\n\nfor i, image_id in enumerate(test_id_list):\n\n    # select the folder_id from the list\n    folder_id = image_id.split('-')[0]\n\n    # set the path to the image\n    path = '../input/matting_human_half/clip_img/' + str(folder_id) + \\\n    '/clip_00000000/' + image_id\n\n    # read the image using skimage\n    image = cv2.imread(path)\n\n    # insert the image into X_train\n    X_test_orig[i] = image\n    \n    \n    \nX_test_orig.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Architecture"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Model, load_model\nfrom keras.layers import Input, UpSampling2D\nfrom keras.layers.core import Dropout, Lambda\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras import backend as K\n\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# source: https://www.kaggle.com/keegil/keras-u-net-starter-lb-0-277\n\n\ninputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n\n# normalize the images\ns = Lambda(lambda x: x / 255) (inputs)\n\nc1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (s)\nc1 = Dropout(0.1) (c1)\nc1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c1)\np1 = MaxPooling2D((2, 2)) (c1)\n\nc2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p1)\nc2 = Dropout(0.1) (c2)\nc2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c2)\np2 = MaxPooling2D((2, 2)) (c2)\n\nc3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p2)\nc3 = Dropout(0.2) (c3)\nc3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c3)\np3 = MaxPooling2D((2, 2)) (c3)\n\nc4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p3)\nc4 = Dropout(0.2) (c4)\nc4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c4)\np4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n\nc5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p4)\nc5 = Dropout(0.3) (c5)\nc5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c5)\n\nu6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same') (c5)\nu6 = concatenate([u6, c4])\nc6 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u6)\nc6 = Dropout(0.2) (c6)\nc6 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c6)\n\nu7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c6)\nu7 = concatenate([u7, c3])\nc7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u7)\nc7 = Dropout(0.2) (c7)\nc7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c7)\n\nu8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c7)\nu8 = concatenate([u8, c2])\nc8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u8)\nc8 = Dropout(0.1) (c8)\nc8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c8)\n\nu9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c8)\nu9 = concatenate([u9, c1], axis=3)\nc9 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u9)\nc9 = Dropout(0.1) (c9)\nc9 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c9)\n\noutputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)\n\nmodel = Model(inputs=[inputs], outputs=[outputs])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Train the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_train_samples = len(df_train)\nnum_val_samples = len(df_val)\ntrain_batch_size = BATCH_SIZE\nval_batch_size = BATCH_SIZE\n\n# determine numtrain steps\ntrain_steps = np.ceil(num_train_samples / train_batch_size)\n# determine num val steps\nval_steps = np.ceil(num_val_samples / val_batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize the generators\ntrain_gen = train_generator(batch_size=BATCH_SIZE)\nval_gen = val_generator(batch_size=BATCH_SIZE)\n\n\nfilepath = \"model.h5\"\n\nearlystopper = EarlyStopping(patience=3, verbose=1)\n\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, \n                             save_best_only=True, mode='min')\n\ncallbacks_list = [earlystopper, checkpoint]\n\nhistory = model.fit_generator(train_gen, steps_per_epoch=train_steps, epochs=20, \n                              validation_data=val_gen, validation_steps=val_steps,\n                             verbose=1,\n                             callbacks=callbacks_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Make a Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make a prediction\n# Note that we have a lambda layer in the model to normalize the images.\n\n# initialize the test generator\ntest_gen = test_generator(batch_size=1)\n\nmodel.load_weights(filepath = 'model.h5')\npredictions = model.predict_generator(test_gen, \n                                      steps=len(df_test),  \n                                      verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Process the predictions"},{"metadata":{},"cell_type":"markdown","source":"### Resize the predictions\n\nIn order to maintain the image sharpness we need to append the alpha channel to the orginal 800x600 images i.e. images that were never resized. This is because a resized image loses its sharpness. Before we can append the predcited alpha channel to an original image we need to resize it from 128x128 to 800x600."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"\n# get the list of images\ntest_id_list = list(df_test['image_id'])\n\n\nfor i, image_id in enumerate(test_id_list):\n    \n    if i == 0:\n        \n        # get a predicted mask\n        image = predictions[i]\n\n        # resize\n        preds = resize(image, (800, 600))\n        \n        # reshape\n        preds = preds.reshape((1, 800, 600, 1))\n    \n    else:\n\n        # get a predicted mask\n        image = predictions[i]\n        \n        # resize\n        image = resize(image, (800, 600))\n        \n        # reshape\n        image = image.reshape((1, 800, 600, 1))\n\n        # stack the images\n        preds = np.vstack((preds, image))\n\n\npreds.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Threshold the predictions"},{"metadata":{},"cell_type":"markdown","source":"The predictions are actually probabilities that a pixel is either part of the person or part of the background. Here we threshold the predictions to convert all values to either 1 or 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Threshold the predictions\n\npreds_test_thresh = (preds >= 0.5).astype(np.uint8)\n\npreds_test_thresh.shape\n\nprint(preds_test_thresh.min())\nprint(preds_test_thresh.max())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Change values from 0/1 to 0/255"},{"metadata":{},"cell_type":"markdown","source":"An alpha channel must have values between 0 and 255."},{"metadata":{"trusted":true},"cell_type":"code","source":"# simply multiply by 255\nalpha_preds = preds_test_thresh * 255\n\nprint(alpha_preds.min())\nprint(alpha_preds.max())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_test_orig.shape)\nprint(alpha_preds.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Concat the predicted alpha channels to the original test images"},{"metadata":{},"cell_type":"markdown","source":"Concatenate alpha_preds to X_test_orig along the channel axis. X_test_orig will then have 4 channels. The 4th channel is the predicted masks with values either 0 or 255.\n\nRemember that the 4th channel controls the transparency of each pixel."},{"metadata":{"trusted":true},"cell_type":"code","source":"# concat\npredicted_masks = np.concatenate((X_test_orig, alpha_preds), axis=-1)\n\npredicted_masks.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Display some results"},{"metadata":{},"cell_type":"markdown","source":"Notice that the predicted images have the same sharpness has the original images. Overall the results look quite good considering that we used only 4000 training images. U-Net was designed for medical applications but it appears that it's also very good at segmenting selfies."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# set up the canvas for the subplots\nplt.figure(figsize=(20,20))\nplt.tight_layout()\nplt.axis('Off')\n\n\n# Our subplot will contain 4 rows and 4 columns\n# plt.subplot(nrows, ncols, plot_number)\n\n# sample fname: 1803281053-00000126.png\n    \n\n# image\nplt.subplot(1,4,1)\nimage = X_test_orig[1,:,:,:] # can also write as X_test_orig[1]\nplt.imshow(image)\nplt.axis('off')\n\n\n# image\nplt.subplot(1,4,2)\nmask = predicted_masks[1, :, :, :] # # can also write as preds_orig[1]\nplt.imshow(mask)\nplt.axis('off')\n\n\n# image\nplt.subplot(1,4,3)\nimage = X_test_orig[2,:,:,:]\nplt.imshow(image)\nplt.axis('off')\n\n\n# image\nplt.subplot(1,4,4)\nmask = predicted_masks[2, :, :, :]\nplt.imshow(mask)\nplt.axis('off')\n\n\n\n# ============ #\n\n\n# set up the canvas for the subplots\nplt.figure(figsize=(20,20))\nplt.tight_layout()\nplt.axis('Off')\n\n\n# image\nplt.subplot(1,4,1)\nimage = X_test_orig[3,:,:,:]\nplt.imshow(image)\nplt.axis('off')\n\n\n# image\nplt.subplot(1,4,2)\nmask = predicted_masks[3, :, :, :]\nplt.imshow(mask)\nplt.axis('off')\n\n\n# image\nplt.subplot(1,4,3)\nimage = X_test_orig[4,:,:,:]\nplt.imshow(image)\nplt.axis('off')\n\n\n# image\nplt.subplot(1,4,4)\nmask = predicted_masks[4, :, :, :]\nplt.imshow(mask)\nplt.axis('off')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Display predictions"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\n# predicted_masks\n\nn = 5\nfig, axs = plt.subplots(nrows=n, ncols=n, sharex=True, sharey=True, figsize=(10,10))\nfor i in range(n**2):\n    ax = axs[i // n, i % n]\n    ax.imshow(predicted_masks[i,:,:,:], cmap=plt.cm.gray)\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Display original test images"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# preds_orig\n\nn = 5\nfig, axs = plt.subplots(nrows=n, ncols=n, sharex=True, sharey=True, figsize=(10,10))\nfor i in range(n**2):\n    ax = axs[i // n, i % n]\n    ax.imshow(X_test_orig[i,:,:,:], cmap=plt.cm.gray)\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reference Kernels"},{"metadata":{},"cell_type":"markdown","source":"- Keras U-Net starter - LB 0.277<br>\nhttps://www.kaggle.com/keegil/keras-u-net-starter-lb-0-277\n\n- Simple Cell Segmentation with Keras and U-Net<br>\nhttps://www.kaggle.com/vbookshelf/simple-cell-segmentation-with-keras-and-u-net\n\n- Python Generators to reduce RAM usage [ Part 2 ]<br>\nhttps://www.kaggle.com/vbookshelf/python-generators-to-reduce-ram-usage-part-2\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion"},{"metadata":{},"cell_type":"markdown","source":"This was a simple approach. To improve the quality of the segmented images you can try the following:\n- training with more images\n- using different loss functions \n- tuning the model parameters\n- using a different model architecture\n\nMany thanks to Laurent H. for making this interesting and high quality dataset available on Kaggle.\n\nThank you for reading."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}